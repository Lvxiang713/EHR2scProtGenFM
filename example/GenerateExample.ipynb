{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ceebfcad-2888-431c-a96e-0e56d8d1d69b",
   "metadata": {},
   "source": [
    "# ▲ This is an exmple for generating sing-cell data conditioned on EHR embbeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3748a706-43ff-4ca3-a51d-313af594505c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "root = os.path.dirname(os.getcwd())\n",
    "sys.path.insert(0, root)\n",
    "import yaml\n",
    "import numpy as np\n",
    "import torch\n",
    "from src.diffusion_src.models.gaussian_diffusion import GaussianDiffusion\n",
    "from src.diffusion_src.models.scAttNet import SingleCellAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8377a8-211c-484c-86fd-fb11409f945d",
   "metadata": {},
   "source": [
    "## Generating data by denoising process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0633809d-6ae0-4f46-9b8c-3d71f78b8683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_cells_chunked(model, cd_dict, gaussian_diffusion, device,\n",
    "                         num_cells_total, cell_num_per_sample,\n",
    "                         feature_num, output_dir):\n",
    "    \"\"\"\n",
    "    Generate synthetic single–cell measurements in chunks for each donor,\n",
    "    saving each donor’s array to a .npy file.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained SingleCellAN model.\n",
    "        cd_dict: Mapping donor_id -> precomputed EHR embedding tensor.\n",
    "        gaussian_diffusion: GaussianDiffusion sampler.\n",
    "        loader: DataLoader over a subset of the dataset (to get donor IDs).\n",
    "        device: torch.device.\n",
    "        num_cells_total: Total number of cells to generate per donor.\n",
    "        cell_num_per_sample: Cells generated per call to diffusion.sample().\n",
    "        feature_num: Dimensionality of each cell feature vector.\n",
    "        output_dir: Directory in which to save .npy files.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    model.eval()\n",
    "    seen = set()\n",
    "    print(\"Genaration process Begin\")\n",
    "    with torch.no_grad():\n",
    "        for did in cd_dict.keys():\n",
    "            if did in seen: continue\n",
    "            seen.add(did)\n",
    "            ehr_emb = cd_dict[did].unsqueeze(1).to(device)\n",
    "            mask    = torch.ones(ehr_emb.size(0), 1, dtype=torch.bool, device=device)\n",
    "            total = []\n",
    "            while len(total) < num_cells_total:\n",
    "                gen = gaussian_diffusion.sample(\n",
    "                    model=model,\n",
    "                    batch_size=1,\n",
    "                    cell_num=cell_num_per_sample,\n",
    "                    dims=feature_num,\n",
    "                    cd=(ehr_emb, mask)\n",
    "                )\n",
    "                total.extend(gen.squeeze(0).cpu().tolist())\n",
    "            arr = np.array(total[:num_cells_total], dtype=np.float32)\n",
    "            np.save(os.path.join(output_dir, f\"{did}.npy\"), arr)\n",
    "            print(f\"generated {did}\")\n",
    "        print(\"Genaration process Finish!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c758ff4-5da7-4254-a2c6-29594ecaea6f",
   "metadata": {},
   "source": [
    "## Load the EHR condition embbeding from contrasive pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "183ebd60-fb64-454a-bf7c-90096fc16cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LvXiang\\AppData\\Local\\Temp\\ipykernel_55136\\1782821601.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cd_dict = torch.load(\"../data/cd_dict.pt\")\n"
     ]
    }
   ],
   "source": [
    "cd_dict = torch.load(\"../data/cd_dict.pt\")\n",
    "cd_dict_v = [v for k,v in cd_dict.items()]\n",
    "emb_dim= cd_dict_v[0].shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a748a8d-dad4-4c0c-9de0-4786b77fd569",
   "metadata": {},
   "source": [
    "## Load the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c159dc9-fe44-4cdd-ac6d-b96a4b9bab87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LvXiang\\AppData\\Local\\Temp\\ipykernel_55136\\407479251.py:36: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  raw = torch.load(os.path.join(\"../checkpoints/diffusion_ckpt\", \"best_diff_model.pth\"),\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SingleCellAN(\n",
       "  (proteinEmb): Sequential(\n",
       "    (0): Linear(in_features=1, out_features=128, bias=True)\n",
       "    (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): SiLU()\n",
       "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (InitEmb): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): SiLU()\n",
       "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (down_blocks): ModuleList(\n",
       "    (0): TimestepEmbedSequential(\n",
       "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (1-2): 2 x TimestepEmbedSequential(\n",
       "      (0): ResidualBlock(\n",
       "        (Linear1): Sequential(\n",
       "          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (time_emb): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "        (Linear2): Sequential(\n",
       "          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (shortcut): Identity()\n",
       "      )\n",
       "    )\n",
       "    (3): TimestepEmbedSequential(\n",
       "      (0): ResidualBlock(\n",
       "        (Linear1): Sequential(\n",
       "          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Linear(in_features=128, out_features=256, bias=True)\n",
       "        )\n",
       "        (time_emb): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "        )\n",
       "        (Linear2): Sequential(\n",
       "          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (shortcut): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (normq): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (normk): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (normv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (att): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (2): CrossAttentionBlock(\n",
       "        (normq): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (prok): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "        (prov): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "        (att): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4-10): 7 x TimestepEmbedSequential(\n",
       "      (0): ResidualBlock(\n",
       "        (Linear1): Sequential(\n",
       "          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (time_emb): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "        )\n",
       "        (Linear2): Sequential(\n",
       "          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (shortcut): Identity()\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (normq): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (normk): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (normv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (att): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (2): CrossAttentionBlock(\n",
       "        (normq): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (prok): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "        (prov): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "        (att): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (time_embed): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "    (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): SiLU()\n",
       "    (3): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (middle_block): TimestepEmbedSequential(\n",
       "    (0): ResidualBlock(\n",
       "      (Linear1): Sequential(\n",
       "        (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (time_emb): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "      )\n",
       "      (Linear2): Sequential(\n",
       "        (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Dropout(p=0.0, inplace=False)\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (shortcut): Identity()\n",
       "    )\n",
       "    (1): AttentionBlock(\n",
       "      (normq): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (normk): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (normv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (att): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (2): CrossAttentionBlock(\n",
       "      (normq): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      (prok): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "      (prov): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "      (att): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (3): ResidualBlock(\n",
       "      (Linear1): Sequential(\n",
       "        (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (time_emb): Sequential(\n",
       "        (0): SiLU()\n",
       "        (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "      )\n",
       "      (Linear2): Sequential(\n",
       "        (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): SiLU()\n",
       "        (2): Dropout(p=0.0, inplace=False)\n",
       "        (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "      (shortcut): Identity()\n",
       "    )\n",
       "  )\n",
       "  (up_blocks): ModuleList(\n",
       "    (0-7): 8 x TimestepEmbedSequential(\n",
       "      (0): ResidualBlock(\n",
       "        (Linear1): Sequential(\n",
       "          (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Linear(in_features=512, out_features=256, bias=True)\n",
       "        )\n",
       "        (time_emb): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=256, bias=True)\n",
       "        )\n",
       "        (Linear2): Sequential(\n",
       "          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (shortcut): Linear(in_features=512, out_features=256, bias=True)\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (normq): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (normk): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (normv): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (att): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (2): CrossAttentionBlock(\n",
       "        (normq): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (prok): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "        (prov): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "        (att): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): TimestepEmbedSequential(\n",
       "      (0): ResidualBlock(\n",
       "        (Linear1): Sequential(\n",
       "          (0): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Linear(in_features=384, out_features=128, bias=True)\n",
       "        )\n",
       "        (time_emb): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "        (Linear2): Sequential(\n",
       "          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (shortcut): Linear(in_features=384, out_features=128, bias=True)\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (normq): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (normk): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (normv): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (att): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (2): CrossAttentionBlock(\n",
       "        (normq): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (prok): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "        (prov): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "        (att): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (9): TimestepEmbedSequential(\n",
       "      (0): ResidualBlock(\n",
       "        (Linear1): Sequential(\n",
       "          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "        )\n",
       "        (time_emb): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "        (Linear2): Sequential(\n",
       "          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (shortcut): Linear(in_features=256, out_features=128, bias=True)\n",
       "      )\n",
       "      (1): AttentionBlock(\n",
       "        (normq): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (normk): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (normv): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (att): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (2): CrossAttentionBlock(\n",
       "        (normq): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (prok): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "        (prov): LayerNorm((36,), eps=1e-05, elementwise_affine=True)\n",
       "        (att): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (10): TimestepEmbedSequential(\n",
       "      (0): ResidualBlock(\n",
       "        (Linear1): Sequential(\n",
       "          (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "        )\n",
       "        (time_emb): Sequential(\n",
       "          (0): SiLU()\n",
       "          (1): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "        (Linear2): Sequential(\n",
       "          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (shortcut): Linear(in_features=256, out_features=128, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (out): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): SiLU()\n",
       "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (4): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (5): SiLU()\n",
       "    (6): Linear(in_features=128, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_cfg(path: str) -> dict:\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "def strip_ddp_prefix(state_dict: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Remove the 'module.' prefix inserted by DDP from each key, so that\n",
    "    the weights can be loaded into a plain nn.Module.\n",
    "    Args:\n",
    "        state_dict: The raw state_dict, possibly with 'module.' prefixes.\n",
    "    Returns:\n",
    "        A new state_dict without the 'module.' prefixes.\n",
    "    \"\"\"\n",
    "    new_state = {}\n",
    "    for k, v in state_dict.items():\n",
    "        if k.startswith(\"module.\"):\n",
    "            new_state[k[7:]] = v\n",
    "        else:\n",
    "            new_state[k] = v\n",
    "    return new_state\n",
    "\n",
    "cfg = load_cfg(os.path.join(\"../configs\", \"diffusion.yaml\"))\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "diff_model = SingleCellAN(\n",
    "    feature_dims=cfg[\"model\"][\"feature_dims\"],\n",
    "    EHR_embdims=emb_dim,\n",
    "    model_dims=cfg[\"model\"][\"model_dims\"],\n",
    "    dims_mult=tuple(cfg[\"model\"][\"dims_mult\"]),\n",
    "    num_res_blocks=cfg[\"model\"][\"num_res_blocks\"],\n",
    "    attention_resolutions=tuple(cfg[\"model\"][\"attention_resolutions\"]),\n",
    "    dropout=cfg[\"model\"][\"dropout\"],\n",
    "    dropoutAtt=cfg[\"model\"][\"dropout_att\"],\n",
    "    num_heads=cfg[\"model\"][\"num_heads\"],\n",
    ").to(device)\n",
    "raw = torch.load(os.path.join(\"../checkpoints/diffusion_ckpt\", \"best_diff_model.pth\"),\n",
    "                 map_location=device)\n",
    "diff_model.load_state_dict(strip_ddp_prefix(raw))\n",
    "diff_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641b725b-747b-4ca3-a970-39aa8f10cdd5",
   "metadata": {},
   "source": [
    "## Generation process (The generation data file is in the output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f31b3b8b-f6d1-4d93-ba36-27c3090e249d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genaration process Begin\n",
      "generated HPAP-050\n",
      "generated HPAP-129\n",
      "generated HPAP-049\n",
      "generated HPAP-044\n",
      "generated HPAP-131\n",
      "generated HPAP-072\n",
      "generated HPAP-146\n",
      "generated HPAP-045\n",
      "generated HPAP-056\n",
      "generated HPAP-139\n",
      "generated HPAP-047\n",
      "generated HPAP-114\n",
      "generated HPAP-135\n",
      "generated HPAP-140\n",
      "generated HPAP-138\n",
      "generated HPAP-087\n",
      "generated HPAP-130\n",
      "generated HPAP-107\n",
      "generated HPAP-055\n",
      "generated HPAP-136\n",
      "generated HPAP-137\n",
      "generated HPAP-113\n",
      "generated HPAP-064\n",
      "generated HPAP-132\n",
      "generated HPAP-122\n",
      "generated HPAP-092\n",
      "generated HPAP-123\n",
      "generated HPAP-043\n",
      "Genaration process Finish!\n"
     ]
    }
   ],
   "source": [
    "gd = GaussianDiffusion()\n",
    "sample_cells_chunked(\n",
    "    model=diff_model,\n",
    "    cd_dict=cd_dict,\n",
    "    gaussian_diffusion=gd,\n",
    "    device=device,\n",
    "    num_cells_total=cfg[\"evaluation\"][\"num_cells_total\"],\n",
    "    cell_num_per_sample=cfg[\"evaluation\"][\"cell_num_per_sample\"],\n",
    "    feature_num=cfg[\"model\"][\"feature_dims\"],\n",
    "    output_dir=cfg[\"evaluation\"][\"sample_dir\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce42d18d-d780-4fcd-8e70-bc595d4452dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff84967-4638-46f9-98b4-53af3dd7c78e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
